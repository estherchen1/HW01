{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of ps2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estherchen1/HW01/blob/master/Copy_of_ps2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSYV3U4JZdzt"
      },
      "source": [
        "## Goals\n",
        "\n",
        "The goals of the coding part of this homework assignment are to:\n",
        " * Practice specifying and fitting linear regression and multinomial regression models in Keras\n",
        " * See how you can do the calculations to generate predictions from these models directly in numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qO0Kphe9bPJk"
      },
      "source": [
        "## Module Imports\n",
        "Please add code below to import numpy and any keras submodules you need.  Set a seed for random number generation from numpy before importing keras.  I've already imported pandas, a function to get train/test splits, and a softmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1FzuhTFCZHoY",
        "outputId": "7b420a52-f301-41a6-c047-a7643c2cb6a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import pandas as pd\n",
        "from scipy.special import softmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(38439784)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import models\n",
        "from keras import Sequential\n",
        "from keras import layers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ht1fJQimoAZT"
      },
      "source": [
        "## Problem 1: House sale price prediction\n",
        "We have a data set with a number of characteristics of houses sold in the city of Ames, IA, as well as the sale price of the house.  Let's fit a model to predict sales price of a house.  (Original data source: De Cock, D. (2011). Journal of Statistics Education, Volume 19, Number 3.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mox46aAzn_jj",
        "outputId": "c3a4e079-63cd-46e1-cd58-bc02ae27eec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# read in data\n",
        "house_prices = pd.read_csv(\"http://www.evanlray.com/data/AmesHousing/AmesHousing.csv\")\n",
        "house_prices = house_prices[['Overall_Qual', 'Overall_Cond', 'Lot_Area',\n",
        "  'Bldg_Type', 'Street', 'Total_Bsmt_SF', 'Heating_QC', 'Gr_Liv_Area',\n",
        "  'Bsmt_Full_Bath', 'Fireplaces', 'Garage_Cars', 'Garage_Area', 'Wood_Deck_SF',\n",
        "  'Year_Built', 'Year_Remod_Add', 'Sale_Price']]\n",
        "house_prices = pd.get_dummies(house_prices, drop_first = True)\n",
        "\n",
        "# how much data do we have?\n",
        "print(\"shape of house_prices = \" + str(house_prices.shape))\n",
        "print(house_prices)\n",
        "\n",
        "house_prices.describe(include = 'all')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of house_prices = (2930, 37)\n",
            "      Lot_Area  Total_Bsmt_SF  ...  Heating_QC_Poor  Heating_QC_Typical\n",
            "0        31770           1080  ...                0                   0\n",
            "1        11622            882  ...                0                   1\n",
            "2        14267           1329  ...                0                   1\n",
            "3        11160           2110  ...                0                   0\n",
            "4        13830            928  ...                0                   0\n",
            "...        ...            ...  ...              ...                 ...\n",
            "2925      7937           1003  ...                0                   1\n",
            "2926      8885            864  ...                0                   1\n",
            "2927     10441            912  ...                0                   1\n",
            "2928     10010           1389  ...                0                   0\n",
            "2929      9627            996  ...                0                   0\n",
            "\n",
            "[2930 rows x 37 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lot_Area</th>\n",
              "      <th>Total_Bsmt_SF</th>\n",
              "      <th>Gr_Liv_Area</th>\n",
              "      <th>Bsmt_Full_Bath</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>Garage_Cars</th>\n",
              "      <th>Garage_Area</th>\n",
              "      <th>Wood_Deck_SF</th>\n",
              "      <th>Year_Built</th>\n",
              "      <th>Year_Remod_Add</th>\n",
              "      <th>Sale_Price</th>\n",
              "      <th>Overall_Qual_Average</th>\n",
              "      <th>Overall_Qual_Below_Average</th>\n",
              "      <th>Overall_Qual_Excellent</th>\n",
              "      <th>Overall_Qual_Fair</th>\n",
              "      <th>Overall_Qual_Good</th>\n",
              "      <th>Overall_Qual_Poor</th>\n",
              "      <th>Overall_Qual_Very_Excellent</th>\n",
              "      <th>Overall_Qual_Very_Good</th>\n",
              "      <th>Overall_Qual_Very_Poor</th>\n",
              "      <th>Overall_Cond_Average</th>\n",
              "      <th>Overall_Cond_Below_Average</th>\n",
              "      <th>Overall_Cond_Excellent</th>\n",
              "      <th>Overall_Cond_Fair</th>\n",
              "      <th>Overall_Cond_Good</th>\n",
              "      <th>Overall_Cond_Poor</th>\n",
              "      <th>Overall_Cond_Very_Good</th>\n",
              "      <th>Overall_Cond_Very_Poor</th>\n",
              "      <th>Bldg_Type_OneFam</th>\n",
              "      <th>Bldg_Type_Twnhs</th>\n",
              "      <th>Bldg_Type_TwnhsE</th>\n",
              "      <th>Bldg_Type_TwoFmCon</th>\n",
              "      <th>Street_Pave</th>\n",
              "      <th>Heating_QC_Fair</th>\n",
              "      <th>Heating_QC_Good</th>\n",
              "      <th>Heating_QC_Poor</th>\n",
              "      <th>Heating_QC_Typical</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "      <td>2930.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>10147.921843</td>\n",
              "      <td>1051.255631</td>\n",
              "      <td>1499.690444</td>\n",
              "      <td>0.431058</td>\n",
              "      <td>0.599317</td>\n",
              "      <td>1.766212</td>\n",
              "      <td>472.658362</td>\n",
              "      <td>93.751877</td>\n",
              "      <td>1971.356314</td>\n",
              "      <td>1984.266553</td>\n",
              "      <td>180796.060068</td>\n",
              "      <td>0.281570</td>\n",
              "      <td>0.077133</td>\n",
              "      <td>0.036519</td>\n",
              "      <td>0.013652</td>\n",
              "      <td>0.205461</td>\n",
              "      <td>0.004437</td>\n",
              "      <td>0.010580</td>\n",
              "      <td>0.119454</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.564505</td>\n",
              "      <td>0.034471</td>\n",
              "      <td>0.013993</td>\n",
              "      <td>0.017065</td>\n",
              "      <td>0.133106</td>\n",
              "      <td>0.003413</td>\n",
              "      <td>0.049147</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>0.827645</td>\n",
              "      <td>0.034471</td>\n",
              "      <td>0.079522</td>\n",
              "      <td>0.021160</td>\n",
              "      <td>0.995904</td>\n",
              "      <td>0.031399</td>\n",
              "      <td>0.162457</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.294881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7880.017759</td>\n",
              "      <td>440.968018</td>\n",
              "      <td>505.508887</td>\n",
              "      <td>0.524762</td>\n",
              "      <td>0.647921</td>\n",
              "      <td>0.761137</td>\n",
              "      <td>215.187196</td>\n",
              "      <td>126.361562</td>\n",
              "      <td>30.245361</td>\n",
              "      <td>20.860286</td>\n",
              "      <td>79886.692357</td>\n",
              "      <td>0.449842</td>\n",
              "      <td>0.266848</td>\n",
              "      <td>0.187609</td>\n",
              "      <td>0.116061</td>\n",
              "      <td>0.404107</td>\n",
              "      <td>0.066473</td>\n",
              "      <td>0.102332</td>\n",
              "      <td>0.324377</td>\n",
              "      <td>0.036930</td>\n",
              "      <td>0.495906</td>\n",
              "      <td>0.182467</td>\n",
              "      <td>0.117482</td>\n",
              "      <td>0.129535</td>\n",
              "      <td>0.339747</td>\n",
              "      <td>0.058331</td>\n",
              "      <td>0.216211</td>\n",
              "      <td>0.048828</td>\n",
              "      <td>0.377753</td>\n",
              "      <td>0.182467</td>\n",
              "      <td>0.270598</td>\n",
              "      <td>0.143943</td>\n",
              "      <td>0.063876</td>\n",
              "      <td>0.174424</td>\n",
              "      <td>0.368933</td>\n",
              "      <td>0.031987</td>\n",
              "      <td>0.456067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1300.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1872.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>12789.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7440.250000</td>\n",
              "      <td>793.000000</td>\n",
              "      <td>1126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1954.000000</td>\n",
              "      <td>1965.000000</td>\n",
              "      <td>129500.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9436.500000</td>\n",
              "      <td>990.000000</td>\n",
              "      <td>1442.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1973.000000</td>\n",
              "      <td>1993.000000</td>\n",
              "      <td>160000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>11555.250000</td>\n",
              "      <td>1301.500000</td>\n",
              "      <td>1742.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>2001.000000</td>\n",
              "      <td>2004.000000</td>\n",
              "      <td>213500.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>215245.000000</td>\n",
              "      <td>6110.000000</td>\n",
              "      <td>5642.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1488.000000</td>\n",
              "      <td>1424.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>755000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Lot_Area  Total_Bsmt_SF  ...  Heating_QC_Poor  Heating_QC_Typical\n",
              "count    2930.000000    2930.000000  ...      2930.000000         2930.000000\n",
              "mean    10147.921843    1051.255631  ...         0.001024            0.294881\n",
              "std      7880.017759     440.968018  ...         0.031987            0.456067\n",
              "min      1300.000000       0.000000  ...         0.000000            0.000000\n",
              "25%      7440.250000     793.000000  ...         0.000000            0.000000\n",
              "50%      9436.500000     990.000000  ...         0.000000            0.000000\n",
              "75%     11555.250000    1301.500000  ...         0.000000            1.000000\n",
              "max    215245.000000    6110.000000  ...         1.000000            1.000000\n",
              "\n",
              "[8 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r3T6yBVx5qYB",
        "outputId": "bf1d5ede-9cc3-497f-d330-80bc80072653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X = house_prices.drop(['Sale_Price'], axis=1).to_numpy()\n",
        "y = house_prices['Sale_Price'].to_numpy().reshape((house_prices.shape[0], 1))\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2930, 36)\n",
            "(2930, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0B6Z_YDHp_oc",
        "outputId": "424fb85d-7e65-4fb0-d619-81f1c6eb6ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "\n",
        "\n",
        "print(y_test.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1875, 36)\n",
            "(586, 36)\n",
            "(469, 36)\n",
            "(586, 1)\n",
            "(1875, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pVwwMAlE7o6N"
      },
      "source": [
        "#### (a) Define an appropriate model in Keras.  Use Adam ('adam') for your optimizer and no additional metrics of performance other than your loss function.  Run the estimation process for about 1000 epochs using a batch size equal to the number of examples in your training set (this will take about a minute).  We'll see what these things mean starting next week.\n",
        "Things to consider:\n",
        "\n",
        "* How many units do you need in your output layer?\n",
        "* What activation function do you want to use?\n",
        "* How many inputs (features) will your network use?\n",
        "* What is the appropriate loss function to use?\n",
        "\n",
        "**Note: the only scenario where you'd use a method like this to fit a basic linear model is if your training set was too large to use other methods.  For a basic regression model there are much faster approaches to parameter estimation based on direct matrix manipulations.**  We're doing this to get practice setting up models in Keras and working with the results.  You'll likely see that when the estimation process stops the loss function is still decreasing, indicating that you have not yet reached a minimum of the negative log-likelihood.  If we really wanted the best-performing model, we'd let estimation keep running.  You can do that if you want to, but there's no need to for the purpose of this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZtlNGkJ270jU",
        "colab": {}
      },
      "source": [
        "my_model = models.Sequential()\n",
        "my_model.add(layers.Dense(units = 1, activation = 'linear', input_shape = (36,)))\n",
        "\n",
        "my_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "my_model.fit(X_train, y_train,\n",
        "  validation_data = (X_val, y_val),\n",
        "  epochs = 1000, batch_size = 1875)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6fQ_2SOADcz2"
      },
      "source": [
        "#### (b) Get an evaluation of the performance of your model on the test set using the built-in Keras function `evaluate`.  Is there evidence that your model has overfit the training data?  (See the solutions for lab 1 or ask if you're not sure what this means.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BP7UU0VX9Sw1",
        "outputId": "ba408523-00ee-42f3-ed78-92f7238cb813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "print(my_model.evaluate(X_test, y_test))\n",
        "\n",
        "y_hat = my_model.predict(X_test)\n",
        "\n",
        "print(\"shape of y hat \" + str(y_hat.shape))\n",
        "print(\"shape of y test\" + str(y_test.shape))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test, y_hat)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "586/586 [==============================] - 0s 41us/step\n",
            "36894693914.2116\n",
            "shape of y hat (586, 1)\n",
            "shape of y test(586, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEICAYAAACTVrmbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmZmuXNwMTOz3Dm4mJlZ7hxczMwsd/8fnUW0irpjbWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_lN7PFwWkxa",
        "colab_type": "text"
      },
      "source": [
        "It is 'overfitting' because the test evaluation is significantly more than the training mse. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TW8QMx_RQFBn"
      },
      "source": [
        "#### (c) Find the test set performance of your model by manually obtaining the predicted values and computing the test set mean squared error based on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ChYWP0y2QTI_",
        "outputId": "65de7094-963b-4eab-ffd9-d10c0317cb06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Extract the w and b estimates\n",
        "(w, b) = my_model.layers[0].get_weights()\n",
        "b = b.reshape(b.shape[0], 1)\n",
        "print(\"b shape = \" + str(b.shape))\n",
        "print(\"w shape = \" + str(w.shape))\n",
        "print(\"X train shape\" + str(X_train.shape))\n",
        "# Calculate a vector of z's\n",
        "z = b + np.dot(w.T, X_train.T) # replace None to the left with an actual calculation\n",
        "\n",
        "print(\"z\" + str(z.shape))\n",
        "\n",
        "# Calculate a vector of a's\n",
        "a = z # replace None to the left with an actual calculation\n",
        "\n",
        "print(\"a\" + str(a.shape))\n",
        "\n",
        "# Calculate and print the test set MSE\n",
        "MSE = y_test - b + np.dot(w.T, x_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b shape = (1, 1)\n",
            "w shape = (36, 1)\n",
            "X train shape(1875, 36)\n",
            "z(1, 1875)\n",
            "a(1, 1875)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiN8hvUMaHO6"
      },
      "source": [
        "## Problem 2: Forest cover type prediction\n",
        "We have a data set from with characteristics of land in national forests; our goal is to predict the type of forest on that land.  There are 7 possible forest types: Spruce/Fir, Lodgepole Pine, Ponderosa Pine, Cottonwood/Willow, Aspen, Douglas-fir, or Krummholz.  Our features are things like elevation, slope of land, distance to water, soil type, and so on.  (Original data source: https://archive.ics.uci.edu/ml/datasets/Covertype)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VNp5PN3ubNcu",
        "outputId": "72ddd8b4-ab4c-4899-8cc7-2ad9652e7d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# read in data\n",
        "forest_cover = pd.read_csv(\"http://www.evanlray.com/data/UCIML/forest_cover/covtype.data\")\n",
        "\n",
        "# drop a couple of columns of all 0's\n",
        "to_drop = [20, 28]\n",
        "forest_cover.drop(forest_cover.columns[to_drop], axis = 1, inplace=True)\n",
        "\n",
        "# how much data do we have?\n",
        "print(\"shape of forest_cover = \" + str(forest_cover.shape))\n",
        "\n",
        "# perform a train/validation/test split\n",
        "# the indices of the split are the same as what was used in the original paper.\n",
        "# note that their test set is much larger than their train set, but you would\n",
        "# never do this in real life.\n",
        "train_slice = slice(0, 11340)\n",
        "val_slice = slice(11340, 11340+3780)\n",
        "test_slice = slice(11340+3780, 11340+3780+565892)\n",
        "\n",
        "X_train = forest_cover.iloc[train_slice, 0:52].to_numpy()\n",
        "y_train = forest_cover.iloc[train_slice, 52].to_numpy() - 1\n",
        "\n",
        "X_val = forest_cover.iloc[val_slice, 0:52].to_numpy()\n",
        "y_val = forest_cover.iloc[val_slice, 52].to_numpy() - 1\n",
        "\n",
        "X_test = forest_cover.iloc[test_slice, 0:52].to_numpy()\n",
        "y_test = forest_cover.iloc[test_slice, 52].to_numpy() - 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of forest_cover = (581011, 53)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qob0HzpLrBKh"
      },
      "source": [
        "#### (a) In lab 1, I claimed that an essential preprocessing step for neural networks is to center and scale the inputs.  That isn't really necessary with just a multiple regression model like in problem 1, but it is necessary as soon as you start using sigmoid or softmax activations -- which you'll need in this problem.  Do the input normalization here.\n",
        "\n",
        "Note that you did this in lab 1, so you could just go find your old code there.  But you might like to try recreating it from scratch first.  (You want to eventually just know how to do things.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h5_iOolnb8Yy",
        "outputId": "bbdcba80-8668-4de3-abad-184a960d2315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "X_train_mean = np.mean(X_train, axis = 0) # add a call to np.mean here.  what will you use for the axis?\n",
        "X_train_std =  np.std(X_train, axis = 0)# add a call to np.std here.  what will you use for the axis?\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_train_mean.shape)\n",
        "print(X_train_std.shape)\n",
        "\n",
        "X_train = X_train - X_train_mean # add code here to subtract the column means from X_train.  How will broadcasting work?\n",
        "X_train = X_train/X_train_std# add code here to divide X_train by the column standard deviations.  How will broadcasting work?\n",
        "\n",
        "# normalize X_val, but using X_train_mean and X_train_std to do the normalization\n",
        "X_val = X_val - X_train_mean # add code here to subtract the column means from X_val\n",
        "X_val = X_val/X_train_std # add code here to divide X_val by the column standard deviations\n",
        "\n",
        "# normalize X_test, but using X_train_mean and X_train_std to do the normalization\n",
        "X_test = X_test - X_train_mean# add code here to subtract the column means from X_test\n",
        "X_test = X_test/X_train_std# add code here to divide X_test by the column standard deviations\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11340, 52)\n",
            "(52,)\n",
            "(52,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZC2-YMnQsarO"
      },
      "source": [
        "#### (b) Define an appropriate model in Keras.  Use stochastic gradient descent ('sgd') for your optimizer and 'accuracy' as an evaluation metric, and run the estimation process for 1000 epochs using a batch size equal to the number of examples in your training set.  We'll see what these things mean starting next week.\n",
        "Things to consider:\n",
        "\n",
        "* How many units do you need in your output layer?\n",
        "* What activation function do you want to use?\n",
        "* How many inputs (features) will your network use?\n",
        "* What is the appropriate loss function to use?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1JZjbKGte8Gj",
        "colab": {}
      },
      "source": [
        "treez = models.Sequential()\n",
        "\n",
        "treez.add(Dense(units = 3, activation = \"relu\", input_shape = (52, )))\n",
        "treez.add(Dense(units = 7, activation = \"softmax\", input_shape = (52,)))\n",
        "\n",
        "treez.compile(optimizer = 'sgd', loss = \"categorical_crossentropy h\", metrics = 'accuracy')\n",
        "\n",
        "\n",
        "treez.fit(X_train, y_train,\n",
        "  validation_data = (X_val, y_val),\n",
        "  epochs = 11340, batch_size = 1875)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nl_1PJeVtnYQ"
      },
      "source": [
        "#### (c) Get an evaluation of the performance of your model on the test set using the built-in Keras function `evaluate`.  Is there evidence that your model has overfit the training data?  (See the solutions for lab 1 or ask me if you're not sure what this means.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EXsuIGyCcOM5",
        "colab": {}
      },
      "source": [
        "print(my_model.evaluate(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tFwZyVntukRF"
      },
      "source": [
        "#### (d) Write your own version of the softmax function according to the docstring in the starter function below.\n",
        "Note that this function expects each observation to be represented in a column of the matrix `z`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KWaImUCSwiZO",
        "colab": {}
      },
      "source": [
        "def my_softmax(z):\n",
        "  '''\n",
        "  Calculate softmax(z) where z is a K by m matrix\n",
        "  \n",
        "  Arguments:\n",
        "   - z, a K by m matrix: the entry in row j and column i of z contains \n",
        "     b_j + w_j^T x^(i), the linear combination of input features for class j and\n",
        "     observation m (we're assuming this computation has already been done for us)\n",
        "  \n",
        "  Return:\n",
        "   - a K by M matrix where column m is calculated as softmax of column m of z\n",
        "  '''\n",
        "  return softmax(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8zn7cK0RxVsN"
      },
      "source": [
        "If you want, you can test your function above out with the following code to make sure it seems to be working (all entries of the result should be non-negative, and the columns should add up to 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dyhSNivSxeIO",
        "colab": {}
      },
      "source": [
        "z = np.array([[17, 5, 2, 7],\n",
        "              [5, 5, 1, 2],\n",
        "              [2, 2, 6, -10]])\n",
        "my_softmax(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jwgj29GFt0B9"
      },
      "source": [
        "#### (e) Find the test set performance of your model by manually obtaining the predicted values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PvTSOP4ToqUO"
      },
      "source": [
        "At some point, you will need to use the softmax function.  Although you wrote your own softmax function above, you should instead use the softmax function from the scipy package that was imported above.  Its implementation is more numerically stable; I'll talk more about this some time later.\n",
        "\n",
        "To find the class with the highest probability based on your `a` array, check out the [argmax](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) function in numpy.  This function will find the index of the largest entry along a given `axis` in a vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3vlQy-69kKbQ",
        "colab": {}
      },
      "source": [
        "# The next two lines extract the estimated weights w and bias b from the model\n",
        "# fit and then reshape b to be a 7 by 1 matrix.\n",
        "(w, b) = multi_logistic_model.layers[0].get_weights()\n",
        "b = b.reshape(b.shape[0], 1)\n",
        "\n",
        "# Calculate the vector z here:\n",
        "# This should involve b, X_test, and w.  Use np.dot() and broadcasting.\n",
        "# Do you need to find any transposes?\n",
        "z = \n",
        "\n",
        "# Calculate the activation a:\n",
        "# Use the softmax function.  The softmax function takes an axis argument.\n",
        "# The axis to use depends on how you set up your calculation for z.\n",
        "a =  # use softmax here\n",
        "\n",
        "# Find the predicted value y hat for each observation\n",
        "y_hat =  # use np.argmax here\n",
        "\n",
        "# For each prediction, determine whether the prediction was correct by\n",
        "# comparing it to the observed test set response.  The result of this\n",
        "# calculation should be a logical vector of the same shape as y_test\n",
        "# that is True for cases where the test set prediction was correct and False\n",
        "# for test set cases where the prediction was wrong.\n",
        "# Careful!  Make sure y_hat and y_test have the same shape before comparing\n",
        "# them, or else you'll accidentally broadcast the comparison and be confused...\n",
        "y_hat_correct_lgl = \n",
        "\n",
        "# Determine what proportion of your test set predictions were correct by\n",
        "# calculating the *mean* of the values in the y_hat_correct_lgl variable.\n",
        "# Note that any True values (correct predictions) are converted to 1 and False\n",
        "# values are converted to 0 when you do this calculation.\n",
        "proportion_correct = \n",
        "\n",
        "# Print the proportion correct so you can see it after running this code cell\n",
        "print(\"Proportion correct = \" + str(proportion_correct))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}